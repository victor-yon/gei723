{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PWTf7H60Z0i_"
   },
   "source": [
    "Backpropagation in spiking neural networks\n",
    "==========================================\n",
    "This notebook presents how the gradient descent algorithm can be adapted for backpropagation in a spiking neural networks with non differentiable activation functions.\n",
    "\n",
    "Copyright (c) 2019, NECOTIS  \n",
    "All rights reserved.  \n",
    "Author: Ismael Balafrej  \n",
    "\n",
    "Work inspired and adapted from \n",
    "1. Surrogate Gradient Learning in Spiking Neural Networks by Zenke & Ganguli (2018) https://arxiv.org/pdf/1901.09948.pdf\n",
    "2. SLAYER: Spike Layer Error Reassignment in Time (2018) https://arxiv.org/pdf/1810.08646.pdf\n",
    "3. Biologically inspired alternatives to backpropagation through time for learning in recurrent neural nets (2019) https://arxiv.org/pdf/1901.09049.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJGOX5B8F7ic"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Le chemin d’accès spécifié est introuvable.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b2ddef89c009>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mquantities\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0munits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCOO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "!pip install quantities sparse > /dev/null\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, model_selection, utils\n",
    "import torch\n",
    "import quantities as units\n",
    "from sparse import COO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "dPyehEEZzc4x",
    "outputId": "d09852c7-2499-43b6-d4de-9f1fcee3dbaa"
   },
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Use the GPU unless there is none available, if you don't have a CUDA enabled GPU, I recommand using Google Colab\n",
    "# available here: https://colab.research.google.com.\n",
    "# Create a new notebook and then go to Runtime -> Change runtime type -> Hardware accelerator -> GPU\n",
    "# This will give you access to a fairly recent GPU for free, for up to 12h continuously\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Let's download the MNIST dataset, available at https://www.openml.org/d/554\n",
    "# You can edit the argument data_home to the directory of your choice.\n",
    "# The dataset will be downloaded there; the default directory is ~/scikit_learn_data/\n",
    "X, y = datasets.fetch_openml('mnist_784', version=1, return_X_y=True, data_home=None)\n",
    "nb_of_samples, nb_of_features = X.shape\n",
    "#X = 70k samples, 28*28 features, y = 70k samples, 1 label (string)\n",
    "\n",
    "# Shuffle the dataset\n",
    "X, y = utils.shuffle(X, y)\n",
    "\n",
    "# Convert the labels (string) to integers for convenience\n",
    "y = np.array(y, dtype=np.int)\n",
    "nb_of_ouputs = np.max(y) + 1\n",
    "\n",
    "# We'll normalize our input data in the range [0., 1[.\n",
    "X = X / pow(2, 8)\n",
    "\n",
    "# And convert the data to a spike train\n",
    "dt = 1*units.ms\n",
    "duration_per_image = 100*units.ms\n",
    "absolute_duration = int(duration_per_image / dt)\n",
    "\n",
    "time_of_spike = (1 - X) * absolute_duration # The brighter the white, the earlier the spike\n",
    "time_of_spike[X < .25] = 0 # \"Remove\" the spikes associated with darker pixels (Presumably less information)\n",
    "\n",
    "sample_id, neuron_idx = np.nonzero(time_of_spike)\n",
    "\n",
    "# We use a sparse COO array to store the spikes for memory requirements\n",
    "# You can use the spike_train variable as if it were a tensor of shape (nb_of_samples, nb_of_features, absolute_duration)\n",
    "spike_train = COO((sample_id, neuron_idx, time_of_spike[sample_id, neuron_idx]),\n",
    "                  np.ones_like(sample_id), shape=(nb_of_samples, nb_of_features, absolute_duration))\n",
    "\n",
    "# We create a 2 layer network (1 hidden, 1 output)\n",
    "nb_hidden = 128 # Number of hidden neurons\n",
    "\n",
    "w1 = torch.empty((nb_of_features, nb_hidden), device=device, dtype=torch.float, requires_grad=True)\n",
    "torch.nn.init.normal_(w1, mean=0., std=.1)\n",
    "\n",
    "w2 = torch.empty((nb_hidden, nb_of_ouputs), device=device, dtype=torch.float, requires_grad=True)\n",
    "torch.nn.init.normal_(w2, mean=0., std=.1)\n",
    "\n",
    "# Split in train/test\n",
    "nb_of_train_samples = int(nb_of_samples * 0.85) # Keep 15% of the dataset for testing\n",
    "train_indices = np.arange(nb_of_train_samples)\n",
    "test_indices = np.arange(nb_of_train_samples, nb_of_samples)\n",
    "\n",
    "class SpikeFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0 # We spike when the (potential-threshold) > 0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone() # Clone will create a copy of the numerical value\n",
    "        grad_input[input < 0] = 0 # The derivative of a ReLU function\n",
    "        return grad_input\n",
    "\n",
    "def run_spiking_layer(input_spike_train, layer_weights, tau_v=20*units.ms, tau_i=5*units.ms, v_threshold=1.0):\n",
    "    \"\"\"Here we implement a current-LIF dynamic in pytorch\"\"\"\n",
    "\n",
    "    # First, we multiply the input spike train by the weights of the current layer to get the current that will be added\n",
    "    # We can calculate this beforehand because the weights are constant in the forward pass (no plasticity)\n",
    "    input_current = torch.einsum(\"abc,bd->adc\", (input_spike_train, layer_weights)) # Equivalent to a matrix multiplication for tensors of dim > 2 using Einstein's Notation\n",
    "\n",
    "    recorded_spikes = [] # Array of the output spikes at each time t\n",
    "    membrane_potential_at_t = torch.zeros((input_spike_train.shape[0], layer_weights.shape[-1]), device=device, dtype=torch.float)\n",
    "    membrane_current_at_t = torch.zeros((input_spike_train.shape[0], layer_weights.shape[-1]), device=device, dtype=torch.float)\n",
    "\n",
    "    for t in range(absolute_duration): # For every timestep\n",
    "        # Apply the leak\n",
    "        membrane_potential_at_t = (1-1/(int(tau_v)))*membrane_potential_at_t  # Using tau_v with euler or exact method\n",
    "        membrane_current_at_t = (1-1/(int(tau_i)))*membrane_current_at_t # Using tau_i with euler or exact method\n",
    "\n",
    "        # Select the input current at time t\n",
    "        input_at_t = input_current[:, :, t]\n",
    "\n",
    "        # Integrate the input current\n",
    "        membrane_current_at_t += input_at_t\n",
    "\n",
    "        # Integrate the input to the membrane potential\n",
    "        membrane_potential_at_t += membrane_current_at_t #/int(tau_v)\n",
    "\n",
    "        # Apply the non-differentiable function\n",
    "        recorded_spikes_at_t = SpikeFunction.apply(membrane_potential_at_t - v_threshold)\n",
    "        recorded_spikes.append(recorded_spikes_at_t)\n",
    "\n",
    "        # Reset the spiked neurons\n",
    "        membrane_potential_at_t[membrane_potential_at_t > v_threshold] = 0\n",
    "\n",
    "    recorded_spikes = torch.stack(recorded_spikes, dim=2) # Stack over time axis (Array -> Tensor)\n",
    "    return recorded_spikes\n",
    "\n",
    "\n",
    "# Set-up training\n",
    "nb_of_epochs = 20\n",
    "batch_size = 256 # The backpropagation is done after every batch, but a batch here is also used for memory requirements \n",
    "number_of_batches = len(train_indices) // batch_size\n",
    "\n",
    "params = [w1, w2] # Trainable parameters\n",
    "optimizer = torch.optim.Adam(params, lr=0.01, amsgrad=True)\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "for e in range(nb_of_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in np.array_split(train_indices, number_of_batches):\n",
    "        # Select batch and convert to tensors\n",
    "        batch_spike_train = torch.FloatTensor(spike_train[batch].todense()).to(device)\n",
    "        batch_labels = torch.LongTensor(y[batch, np.newaxis]).to(device)\n",
    "\n",
    "        # Here we create a target spike count (10 spikes for wrong label, 100 spikes for true label) in a one-hot fashion\n",
    "        # This approach is seen in Shrestha & Orchard (2018) https://arxiv.org/pdf/1810.08646.pdf\n",
    "        # Code available at https://github.com/bamsumit/slayerPytorch\n",
    "        min_spike_count = 10 * torch.ones((batch.shape[0], 10), device=device, dtype=torch.float)\n",
    "        target_output = min_spike_count.scatter_(1, batch_labels, 100.0)\n",
    "\n",
    "        # Forward propagation\n",
    "        layer_1_spikes = run_spiking_layer(batch_spike_train, w1)\n",
    "        layer_2_spikes = run_spiking_layer(layer_1_spikes, w2)\n",
    "        network_output = torch.sum(layer_2_spikes, 2) # Count the spikes over time axis\n",
    "        loss = loss_fn(network_output, target_output)\n",
    "\n",
    "        # Backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(\"Epoch %i -- loss : %.4f\" %(e+1, epoch_loss / number_of_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "NgQyFnPetmxQ",
    "outputId": "371b69dc-2c0d-40d6-b46a-b40a128f2c17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on test set: 0.869\n"
     ]
    }
   ],
   "source": [
    "# Test the accuracy of the model\n",
    "correct_label_count = 0\n",
    "# We only need to batchify the test set for memory requirements\n",
    "for batch in np.array_split(test_indices,  len(test_indices) // batch_size):\n",
    "    test_spike_train = torch.FloatTensor(spike_train[batch].todense()).to(device)\n",
    "  \n",
    "    # Same forward propagation as before\n",
    "    layer_1_spikes = run_spiking_layer(test_spike_train, w1)\n",
    "    layer_2_spikes = run_spiking_layer(layer_1_spikes, w2)\n",
    "    network_output = torch.sum(layer_2_spikes, 2) # Count the spikes over time axis\n",
    "    \n",
    "    # Do the prediction by selecting the output neuron with the most number of spikes\n",
    "    _, am = torch.max(network_output, 1) \n",
    "    correct_label_count += np.sum(am.detach().cpu().numpy() == y[batch])\n",
    "\n",
    "print(\"Model accuracy on test set: %.3f\" % (correct_label_count / len(test_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fo_9XpqikN4v"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Code a trous: Backprop in SNN with Pytorch",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
