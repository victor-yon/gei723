{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code a trous: Backprop in SNN with Pytorch",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWTf7H60Z0i_",
        "colab_type": "text"
      },
      "source": [
        "Backpropagation in spiking neural networks\n",
        "==========================================\n",
        "This notebook presents how the gradient descent algorithm can be adapted for backpropagation in a spiking neural networks with non differentiable activation functions.\n",
        "\n",
        "Copyright (c) 2019, NECOTIS  \n",
        "All rights reserved.  \n",
        "Author: Ismael Balafrej  \n",
        "\n",
        "Work inspired and adapted from \n",
        "1. Surrogate Gradient Learning in Spiking Neural Networks by Zenke & Ganguli (2018) https://arxiv.org/pdf/1901.09948.pdf\n",
        "2. SLAYER: Spike Layer Error Reassignment in Time (2018) https://arxiv.org/pdf/1810.08646.pdf\n",
        "3. Biologically inspired alternatives to backpropagation through time for learning in recurrent neural nets (2019) https://arxiv.org/pdf/1901.09049.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJGOX5B8F7ic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install quantities sparse > /dev/null\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets, model_selection, utils\n",
        "import torch\n",
        "import quantities as units\n",
        "from sparse import COO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d09852c7-2499-43b6-d4de-9f1fcee3dbaa",
        "id": "dPyehEEZzc4x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "# Reproducibility\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "# Use the GPU unless there is none available, if you don't have a CUDA enabled GPU, I recommand using Google Colab\n",
        "# available here: https://colab.research.google.com.\n",
        "# Create a new notebook and then go to Runtime -> Change runtime type -> Hardware accelerator -> GPU\n",
        "# This will give you access to a fairly recent GPU for free, for up to 12h continuously\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# Let's download the MNIST dataset, available at https://www.openml.org/d/554\n",
        "# You can edit the argument data_home to the directory of your choice.\n",
        "# The dataset will be downloaded there; the default directory is ~/scikit_learn_data/\n",
        "X, y = datasets.fetch_openml('mnist_784', version=1, return_X_y=True, data_home=None)\n",
        "nb_of_samples, nb_of_features = X.shape\n",
        "#X = 70k samples, 28*28 features, y = 70k samples, 1 label (string)\n",
        "\n",
        "# Shuffle the dataset\n",
        "X, y = utils.shuffle(X, y)\n",
        "\n",
        "# Convert the labels (string) to integers for convenience\n",
        "y = np.array(y, dtype=np.int)\n",
        "nb_of_ouputs = np.max(y) + 1\n",
        "\n",
        "# We'll normalize our input data in the range [0., 1[.\n",
        "X = X / pow(2, 8)\n",
        "\n",
        "# And convert the data to a spike train\n",
        "dt = 1*units.ms\n",
        "duration_per_image = 100*units.ms\n",
        "absolute_duration = int(duration_per_image / dt)\n",
        "\n",
        "time_of_spike = (1 - X) * absolute_duration # The brighter the white, the earlier the spike\n",
        "time_of_spike[X < .25] = 0 # \"Remove\" the spikes associated with darker pixels (Presumably less information)\n",
        "\n",
        "sample_id, neuron_idx = np.nonzero(time_of_spike)\n",
        "\n",
        "# We use a sparse COO array to store the spikes for memory requirements\n",
        "# You can use the spike_train variable as if it were a tensor of shape (nb_of_samples, nb_of_features, absolute_duration)\n",
        "spike_train = COO((sample_id, neuron_idx, time_of_spike[sample_id, neuron_idx]),\n",
        "                  np.ones_like(sample_id), shape=(nb_of_samples, nb_of_features, absolute_duration))\n",
        "\n",
        "# We create a 2 layer network (1 hidden, 1 output)\n",
        "nb_hidden = 128 # Number of hidden neurons\n",
        "\n",
        "w1 = torch.empty((nb_of_features, nb_hidden), device=device, dtype=torch.float, requires_grad=True)\n",
        "torch.nn.init.normal_(w1, mean=0., std=.1)\n",
        "\n",
        "w2 = torch.empty((nb_hidden, nb_of_ouputs), device=device, dtype=torch.float, requires_grad=True)\n",
        "torch.nn.init.normal_(w2, mean=0., std=.1)\n",
        "\n",
        "# Split in train/test\n",
        "nb_of_train_samples = int(nb_of_samples * 0.85) # Keep 15% of the dataset for testing\n",
        "train_indices = np.arange(nb_of_train_samples)\n",
        "test_indices = np.arange(nb_of_train_samples, nb_of_samples)\n",
        "\n",
        "class SpikeFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        ctx.save_for_backward(input)\n",
        "        out = torch.zeros_like(input)\n",
        "        out[input > 0] = 1.0 # We spike when the (potential-threshold) > 0\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, = ctx.saved_tensors\n",
        "        grad_input = grad_output.clone() # Clone will create a copy of the numerical value\n",
        "        grad_input[input < 0] = 0 # The derivative of a ReLU function\n",
        "        return grad_input\n",
        "\n",
        "def run_spiking_layer(input_spike_train, layer_weights, tau_v=20*units.ms, tau_i=5*units.ms, v_threshold=1.0):\n",
        "    \"\"\"Here we implement a current-LIF dynamic in pytorch\"\"\"\n",
        "\n",
        "    # First, we multiply the input spike train by the weights of the current layer to get the current that will be added\n",
        "    # We can calculate this beforehand because the weights are constant in the forward pass (no plasticity)\n",
        "    input_current = torch.einsum(\"abc,bd->adc\", (input_spike_train, layer_weights)) # Equivalent to a matrix multiplication for tensors of dim > 2 using Einstein's Notation\n",
        "\n",
        "    recorded_spikes = [] # Array of the output spikes at each time t\n",
        "    membrane_potential_at_t = torch.zeros((input_spike_train.shape[0], layer_weights.shape[-1]), device=device, dtype=torch.float)\n",
        "    membrane_current_at_t = torch.zeros((input_spike_train.shape[0], layer_weights.shape[-1]), device=device, dtype=torch.float)\n",
        "\n",
        "    for t in range(absolute_duration): # For every timestep\n",
        "        # Apply the leak\n",
        "        membrane_potential_at_t = toBeCompleted # Using tau_v with euler or exact method\n",
        "        membrane_current_at_t = toBeCompleted # Using tau_i with euler or exact method\n",
        "\n",
        "        # Select the input current at time t\n",
        "        input_at_t = input_current[:, :, t]\n",
        "\n",
        "        # Integrate the input current\n",
        "        membrane_current_at_t += input_at_t\n",
        "\n",
        "        # Integrate the input to the membrane potential\n",
        "        membrane_potential_at_t += membrane_current_at_t\n",
        "\n",
        "        # Apply the non-differentiable function\n",
        "        recorded_spikes_at_t = SpikeFunction.apply(membrane_potential_at_t - v_threshold)\n",
        "        recorded_spikes.append(recorded_spikes_at_t)\n",
        "\n",
        "        # Reset the spiked neurons\n",
        "        membrane_potential_at_t[membrane_potential_at_t > v_threshold] = 0\n",
        "\n",
        "    recorded_spikes = torch.stack(recorded_spikes, dim=2) # Stack over time axis (Array -> Tensor)\n",
        "    return recorded_spikes\n",
        "\n",
        "\n",
        "# Set-up training\n",
        "nb_of_epochs = 20\n",
        "batch_size = 256 # The backpropagation is done after every batch, but a batch here is also used for memory requirements \n",
        "number_of_batches = len(train_indices) // batch_size\n",
        "\n",
        "params = [w1, w2] # Trainable parameters\n",
        "optimizer = torch.optim.Adam(params, lr=0.01, amsgrad=True)\n",
        "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
        "\n",
        "for e in range(nb_of_epochs):\n",
        "    epoch_loss = 0\n",
        "    for batch in np.array_split(train_indices, number_of_batches):\n",
        "        # Select batch and convert to tensors\n",
        "        batch_spike_train = torch.FloatTensor(spike_train[batch].todense()).to(device)\n",
        "        batch_labels = torch.LongTensor(y[batch, np.newaxis]).to(device)\n",
        "\n",
        "        # Here we create a target spike count (10 spikes for wrong label, 100 spikes for true label) in a one-hot fashion\n",
        "        # This approach is seen in Shrestha & Orchard (2018) https://arxiv.org/pdf/1810.08646.pdf\n",
        "        # Code available at https://github.com/bamsumit/slayerPytorch\n",
        "        min_spike_count = 10 * torch.ones((batch.shape[0], 10), device=device, dtype=torch.float)\n",
        "        target_output = min_spike_count.scatter_(1, batch_labels, 100.0)\n",
        "\n",
        "        # Forward propagation\n",
        "        layer_1_spikes = run_spiking_layer(batch_spike_train, w1)\n",
        "        layer_2_spikes = run_spiking_layer(layer_1_spikes, w2)\n",
        "        network_output = torch.sum(layer_2_spikes, 2) # Count the spikes over time axis\n",
        "        loss = loss_fn(network_output, target_output)\n",
        "\n",
        "        # Backward propagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(\"Epoch %i -- loss : %.4f\" %(e+1, epoch_loss / number_of_batches))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 -- loss : 328.7854\n",
            "Epoch 2 -- loss : 280.9454\n",
            "Epoch 3 -- loss : 274.2292\n",
            "Epoch 4 -- loss : 272.9392\n",
            "Epoch 5 -- loss : 272.9115\n",
            "Epoch 6 -- loss : 272.9176\n",
            "Epoch 7 -- loss : 272.7327\n",
            "Epoch 8 -- loss : 272.4144\n",
            "Epoch 9 -- loss : 272.1312\n",
            "Epoch 10 -- loss : 271.8605\n",
            "Epoch 11 -- loss : 271.4815\n",
            "Epoch 12 -- loss : 270.7438\n",
            "Epoch 13 -- loss : 270.3649\n",
            "Epoch 14 -- loss : 270.3311\n",
            "Epoch 15 -- loss : 270.2173\n",
            "Epoch 16 -- loss : 270.2439\n",
            "Epoch 17 -- loss : 270.1217\n",
            "Epoch 18 -- loss : 270.1146\n",
            "Epoch 19 -- loss : 270.0847\n",
            "Epoch 20 -- loss : 270.1059\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgQyFnPetmxQ",
        "colab_type": "code",
        "outputId": "371b69dc-2c0d-40d6-b46a-b40a128f2c17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Test the accuracy of the model\n",
        "correct_label_count = 0\n",
        "# We only need to batchify the test set for memory requirements\n",
        "for batch in np.array_split(test_indices,  len(test_indices) // batch_size):\n",
        "    test_spike_train = torch.FloatTensor(spike_train[batch].todense()).to(device)\n",
        "  \n",
        "    # Same forward propagation as before\n",
        "    layer_1_spikes = run_spiking_layer(test_spike_train, w1)\n",
        "    layer_2_spikes = run_spiking_layer(layer_1_spikes, w2)\n",
        "    network_output = torch.sum(layer_2_spikes, 2) # Count the spikes over time axis\n",
        "    \n",
        "    # Do the prediction by selecting the output neuron with the most number of spikes\n",
        "    _, am = torch.max(network_output, 1) \n",
        "    correct_label_count += np.sum(am.detach().cpu().numpy() == y[batch])\n",
        "\n",
        "print(\"Model accuracy on test set: %.3f\" % (correct_label_count / len(test_indices)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model accuracy on test set: 0.869\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo_9XpqikN4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}